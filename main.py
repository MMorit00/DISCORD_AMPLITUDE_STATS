import os
import io
import sys
import json
import gzip
import math
import zipfile
import logging
import datetime as dt
from typing import Dict, Tuple, Set, Optional
import requests
from dateutil import tz

# æ—¥å¿—
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format="%(asctime)s %(levelname)s %(message)s",
)

# ç¯å¢ƒå˜é‡
AMPLITUDE_API_KEY = os.getenv("AMPLITUDE_API_KEY")
AMPLITUDE_SECRET_KEY = os.getenv("AMPLITUDE_SECRET_KEY")
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")
TIMEZONE = os.getenv("AMPLITUDE_TIMEZONE", "UTC")
# è§¦å‘æ¨¡å¼ï¼šSCHEDULEDï¼ˆå®šæ—¶è§¦å‘ï¼ŒæŸ¥ä¸Šå‘¨æ•°æ®ï¼‰æˆ– MANUALï¼ˆæ‰‹åŠ¨è§¦å‘ï¼ŒæŸ¥ä»Šæ—¥æ•°æ®ï¼‰
TRIGGER_MODE = os.getenv("TRIGGER_MODE", "MANUAL").upper()
# åŒºåŸŸï¼ˆå¯é€‰ï¼‰ï¼šUS æˆ– EUã€‚è‹¥è®¾ç½®äº† AMPLITUDE_BASE_URLï¼Œåˆ™å¿½ç•¥æœ¬é¡¹
AMPLITUDE_REGION = os.getenv("AMPLITUDE_REGION", "").strip().upper()
# å›ºå®šä½¿ç”¨ https://analytics.amplitude.com
# æ³¨æ„ï¼šæ­¤åŸŸåå¯èƒ½ä¸æ˜¯æ­£ç¡®çš„ Export API ç«¯ç‚¹ï¼Œå¦‚é‡ 403 é”™è¯¯ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨å°è¯•åˆ‡æ¢åˆ°æ­£ç¡®çš„åŸŸå
AMPLITUDE_BASE_URL = os.getenv("AMPLITUDE_BASE_URL") or "https://analytics.amplitude.com"

def _normalize_base_url(u: str) -> str:
    if not u:
        return "https://amplitude.com"
    u = u.strip()
    if u.endswith("/"):
        u = u[:-1]
    if not (u.startswith("http://") or u.startswith("https://")):
        u = "https://" + u
    return u

def _alternate_base_url(u: str) -> str:
    """
    è¿”å›å¦ä¸€åŒºåŸŸçš„åŸºç¡€åŸŸåï¼ˆUS <-> EUï¼‰
    """
    if "eu.amplitude.com" in u:
        return "https://amplitude.com"
    return "https://analytics.eu.amplitude.com"

AMPLITUDE_BASE_URL = _normalize_base_url(AMPLITUDE_BASE_URL)

if not (AMPLITUDE_API_KEY and AMPLITUDE_SECRET_KEY and DISCORD_WEBHOOK_URL):
    logging.error("ç¼ºå°‘å¿…è¦ç¯å¢ƒå˜é‡ï¼šAMPLITUDE_API_KEY / AMPLITUDE_SECRET_KEY / DISCORD_WEBHOOK_URL")
    sys.exit(1)

def get_time_range(tz_name: str = "UTC", mode: str = "MANUAL") -> Tuple[dt.datetime, dt.datetime, str]:
    """
    æ ¹æ®è§¦å‘æ¨¡å¼è¿”å›æ—¶é—´èŒƒå›´
    SCHEDULED: è¿”å›ä¸Šå‘¨æ•°æ®ï¼ˆä¸Šå‘¨ä¸€00:00åˆ°ä¸Šå‘¨æ—¥23:59ï¼‰
    MANUAL: è¿”å›ä»Šæ—¥æ•°æ®ï¼ˆä»Šæ—¥00:00åˆ°ç°åœ¨ï¼‰
    """
    tzinfo = tz.gettz(tz_name)
    now = dt.datetime.now(tzinfo)
    
    if mode == "SCHEDULED":
        # å®šæ—¶è§¦å‘ï¼šæŸ¥ä¸Šå‘¨æ•°æ®ï¼ˆä¸Šå‘¨ä¸€åˆ°ä¸Šå‘¨æ—¥ï¼‰
        weekday = now.weekday()  # Monday=0
        this_monday = (now - dt.timedelta(days=weekday)).replace(hour=0, minute=0, second=0, microsecond=0)
        last_monday = this_monday - dt.timedelta(days=7)
        last_sunday = this_monday - dt.timedelta(seconds=1)
        
        return last_monday, last_sunday, "ä¸Šå‘¨"
    else:
        # æ‰‹åŠ¨è§¦å‘ï¼šæŸ¥ä»Šæ—¥æ•°æ®
        today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
        return today_start, now, "ä»Šæ—¥"

def amplitude_export(start: dt.datetime, end: dt.datetime) -> bytes:
    """
    è°ƒç”¨ Amplitude Export APIï¼Œè¿”å› zip äºŒè¿›åˆ¶æ•°æ®
    æ–‡æ¡£ï¼š{BASE}/api/2/export?start=YYYYMMDDTHH&end=YYYYMMDDTHHï¼ˆUTC å°æ—¶ç²’åº¦ï¼Œend ä¸ºå¼€åŒºé—´ï¼‰
    - æŸäº›ç§Ÿæˆ·/æ—¶é—´æ®µå¯èƒ½è¿”å› 404ï¼Œè¡¨ç¤ºæ— å¯å¯¼å‡ºå½’æ¡£ï¼›æ­¤æ—¶æŒ‰â€œå¤©â€æ‹†åˆ†é‡è¯•å¹¶åˆå¹¶ã€‚
    - è‹¥å‡­æ®/åŒºåŸŸä¸åŒ¹é…ï¼Œå¯èƒ½è¿”å› 401/403 æˆ– HTML ç™»å½•é¡µï¼›æ­¤æ—¶è‡ªåŠ¨åˆ‡æ¢å¦ä¸€åŒºåŸŸé‡è¯•ä¸€æ¬¡ã€‚
    """
    start_utc = start.astimezone(tz.UTC)
    end_utc = end.astimezone(tz.UTC)
    fmt = "%Y%m%dT%H"

    auth = (AMPLITUDE_API_KEY, AMPLITUDE_SECRET_KEY)
    params = {"start": start_utc.strftime(fmt), "end": end_utc.strftime(fmt)}
    headers = {"Accept": "application/zip"}

    def _request_with_base(base_url: str):
        url = f"{base_url}/api/2/export"
        logging.info(f"è¯·æ±‚ Amplitude Export: {url} params={params}")
        r0 = requests.get(url, params=params, auth=auth, headers=headers, stream=True, timeout=600, allow_redirects=False)
        return url, r0

    # ç¬¬ä¸€æ¬¡ä»¥é…ç½®åŒºåŸŸè¯·æ±‚
    base_used = AMPLITUDE_BASE_URL
    url, r = _request_with_base(base_used)

    # å¦‚é‡ 401/403 æˆ–è¢«é‡å®šå‘è‡³ç™»å½•ï¼ˆ3xxï¼‰æˆ–è¿”å› HTMLï¼ˆç–‘ä¼¼ç™»å½•é¡µï¼‰ï¼Œå°è¯•å¦ä¸€åŒºåŸŸ
    ct = r.headers.get("Content-Type", "").lower()
    if r.status_code in (401, 403) or (300 <= r.status_code < 400) or (r.status_code == 200 and "text/html" in ct):
        alt_base = _normalize_base_url(_alternate_base_url(base_used))
        logging.warning("Export è¿”å›å¼‚å¸¸çŠ¶æ€ %sï¼ˆæˆ–ç–‘ä¼¼ç™»å½•é¡µï¼‰ï¼Œå°è¯•åˆ‡æ¢åŒºåŸŸï¼š%s -> %s", r.status_code, base_used, alt_base)
        base_used = alt_base
        url, r = _request_with_base(base_used)
        if 200 <= r.status_code < 300:
            logging.warning("åˆ‡æ¢åŒºåŸŸåæˆåŠŸã€‚å»ºè®®å°† AMPLITUDE_BASE_URL å›ºå®šä¸ºï¼š%sï¼ˆæˆ–è®¾ç½® AMPLITUDE_REGION å¯¹åº” US/EUï¼‰", base_used)

    if r.status_code == 404:
        # åˆ†å¤©é‡è¯•å¹¶åˆå¹¶ zip å†…å®¹
        logging.warning("Export è¿”å› 404ï¼Œå°è¯•æŒ‰å¤©åˆ†æ®µé‡è¯•")
        buf = io.BytesIO()
        wrote_any = False
        with zipfile.ZipFile(buf, "w") as combined_zip:
            cur = start_utc
            while cur < end_utc:
                day_start = cur
                day_end = min(cur + dt.timedelta(days=1), end_utc)
                day_params = {"start": day_start.strftime(fmt), "end": day_end.strftime(fmt)}
                logging.info(f"æŒ‰å¤©é‡è¯•ï¼š{day_params}")
                day_url = f"{base_used}/api/2/export"
                dr = requests.get(day_url, params=day_params, auth=auth, headers=headers, stream=True, timeout=600)
                if dr.status_code == 404:
                    logging.info(f"è¯¥æ—¥æ— å½’æ¡£ï¼š{day_params}")
                    cur = day_end
                    continue
                dr.raise_for_status()
                with zipfile.ZipFile(io.BytesIO(dr.content)) as dz:
                    for name in dz.namelist():
                        combined_zip.writestr(name, dz.read(name))
                        wrote_any = True
                cur = day_end
        if not wrote_any:
            raise FileNotFoundError("è¯¥æ—¶é—´æ®µæ²¡æœ‰å¯å¯¼å‡ºçš„ Amplitude æ•°æ®ï¼ˆå¤šæ¬¡ 404ï¼‰ã€‚è¯·ç¡®è®¤æ•°æ®ä¸­å¿ƒåŸŸåä¸æ—¶é—´åŒºé—´ã€‚")
        return buf.getvalue()

    # é 404 çš„é”™è¯¯ç›´æ¥æŠ›å‡ºï¼Œä¾¿äºä¸Šå±‚æ•è·å¹¶å‘åˆ° Discord
    r.raise_for_status()
    return r.content  # zip å†…å®¹

def parse_events_from_zip(zip_bytes: bytes):
    """
    è§£æ zip å†…çš„ .json.gz æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªäº‹ä»¶ JSON
    """
    with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zf:
        for name in zf.namelist():
            if not name.endswith(".json.gz"):
                continue
            with zf.open(name) as gz_file:
                with gzip.GzipFile(fileobj=io.BufferedReader(gz_file)) as jf:
                    for line in jf:
                        if not line:
                            continue
                        try:
                            yield json.loads(line)
                        except Exception as e:
                            logging.warning(f"è§£æè¡Œå¤±è´¥: {e}")

def aggregate_week(events_iter) -> Dict[str, int]:
    """
    æŒ‡æ ‡ï¼š
    - å‘¨æ´»è·ƒç”¨æˆ·ï¼šä¼˜å…ˆ user_idï¼Œå¦åˆ™ device_id å»é‡
    - äº‹ä»¶æ•°ï¼šæ€»æ•°
    """
    users: Set[str] = set()
    event_count = 0
    for ev in events_iter:
        event_count += 1
        uid = ev.get("user_id") or ev.get("device_id")
        if uid:
            users.add(str(uid))
    return {"æ´»è·ƒç”¨æˆ·": len(users), "äº‹ä»¶æ•°": event_count}

def calculate_growth(curr: Dict[str, int], prev: Dict[str, int]) -> Dict[str, Optional[float]]:
    def rate(c, p):
        if not p:
            return None
        return (c - p) / p * 100.0
    return {
        "æ´»è·ƒç”¨æˆ·å¢é•¿ç‡": rate(curr.get("æ´»è·ƒç”¨æˆ·", 0), prev.get("æ´»è·ƒç”¨æˆ·", 0)),
        "äº‹ä»¶æ•°å¢é•¿ç‡": rate(curr.get("äº‹ä»¶æ•°", 0), prev.get("äº‹ä»¶æ•°", 0)),
    }

def fmt_pct(v: Optional[float]) -> str:
    if v is None or (isinstance(v, float) and (v != v)):  # NaN
        return "N/A"
    sign = "+" if v > 0 else ""
    return f"{sign}{v:.2f}%"

def build_message(prev_week_label: str, prev_stats: Dict[str, int],
                  last_week_label: str, last_stats: Dict[str, int],
                  growth: Dict[str, Optional[float]]) -> str:
    lines = []
    lines.append("Amplitude æ—¥ç»Ÿè®¡æ•°æ®")
    lines.append("")
    lines.append(f"{prev_week_label}ï¼š")
    lines.append(f"- æ´»è·ƒç”¨æˆ·ï¼š{prev_stats['æ´»è·ƒç”¨æˆ·']}")
    lines.append(f"- äº‹ä»¶æ•°ï¼š{prev_stats['äº‹ä»¶æ•°']}")
    lines.append("")
    lines.append(f"{last_week_label}ï¼š")
    lines.append(f"- æ´»è·ƒç”¨æˆ·ï¼š{last_stats['æ´»è·ƒç”¨æˆ·']}")
    lines.append(f"- äº‹ä»¶æ•°ï¼š{last_stats['äº‹ä»¶æ•°']}")
    lines.append("")
    lines.append("æ—¥ç¯æ¯”ï¼š")
    lines.append(f"- æ´»è·ƒç”¨æˆ·ï¼š{fmt_pct(growth['æ´»è·ƒç”¨æˆ·å¢é•¿ç‡'])}")
    lines.append(f"- äº‹ä»¶æ•°ï¼š{fmt_pct(growth['äº‹ä»¶æ•°å¢é•¿ç‡'])}")
    return "\n".join(lines)

def post_to_discord(webhook_url: str, content: str):
    r = requests.post(webhook_url, json={"content": content}, timeout=30)
    try:
        r.raise_for_status()
        logging.info("å·²å‘é€åˆ° Discord")
    except Exception:
        logging.error("å‘é€åˆ° Discord å¤±è´¥ï¼š%s %s", r.status_code, r.text)
        raise

def main():
    tzinfo = tz.gettz(TIMEZONE)
    
    # æ ¹æ®è§¦å‘æ¨¡å¼è·å–æ—¶é—´èŒƒå›´
    start_time, end_time, period_label = get_time_range(tz_name=TIMEZONE, mode=TRIGGER_MODE)

    def format_time_range(s: dt.datetime, e: dt.datetime):
        s_local = s.astimezone(tzinfo)
        e_local = e.astimezone(tzinfo)
        if s_local.date() == e_local.date():
            return f"{s_local.strftime('%Y-%m-%d')} ({s_local.strftime('%H:%M')} - {e_local.strftime('%H:%M')})"
        return f"{s_local.strftime('%Y-%m-%d %H:%M')} ~ {e_local.strftime('%Y-%m-%d %H:%M')}"

    time_range_str = format_time_range(start_time, end_time)
    
    logging.info(f"è§¦å‘æ¨¡å¼ï¼š{TRIGGER_MODE}")
    logging.info(f"ä¸‹è½½{period_label}æ•°æ®ï¼š{time_range_str}")
    
    data_zip = amplitude_export(start_time, end_time)
    stats = aggregate_week(parse_events_from_zip(data_zip))

    # æ„å»ºæŠ¥å‘Šæ¶ˆæ¯ï¼ˆç¾åŒ–ï¼šåˆ†å‰²çº¿ + emojiï¼‰
    lines = []
    lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    if TRIGGER_MODE == "SCHEDULED":
        lines.append("ğŸ“Š Amplitude ä¸Šå‘¨ç»Ÿè®¡")
    else:
        lines.append("ğŸ“Š Amplitude ä»Šæ—¥ç»Ÿè®¡")
    lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    lines.append(f"ğŸ—“ï¸ {period_label}ï¼ˆ{time_range_str}ï¼‰")
    lines.append("")
    lines.append(f"ğŸ‘¥ æ´»è·ƒç”¨æˆ·ï¼š**{stats['æ´»è·ƒç”¨æˆ·']:,}**")
    lines.append(f"ğŸ¯ äº‹ä»¶æ•°ï¼š**{stats['äº‹ä»¶æ•°']:,}**")
    lines.append("")
    lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    message = "\n".join(lines)

    logging.info("å‘é€æ–‡æœ¬åˆ° Discord")
    post_to_discord(DISCORD_WEBHOOK_URL, message)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        err = f"Amplitude å‘¨æŠ¥ä»»åŠ¡å¤±è´¥ï¼š{e}"
        logging.exception(err)
        try:
            if DISCORD_WEBHOOK_URL:
                post_to_discord(DISCORD_WEBHOOK_URL, err)
        finally:
            sys.exit(1)
